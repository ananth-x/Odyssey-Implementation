{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_A4lRslt2Anm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'python3' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!python3 --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IFkchm5W2Ano"
      },
      "outputs": [],
      "source": [
        "\n",
        "def interpret_vit(image, text, model, device, index=None):\n",
        "    logits_per_image, logits_per_text = model(image, text)\n",
        "    print(logits_per_image)\n",
        "    probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()\n",
        "    if index is None:\n",
        "        index = np.argmax(logits_per_image.cpu().data.numpy(), axis=-1)\n",
        "    one_hot = np.zeros((1, logits_per_image.size()[-1]), dtype=np.float32)\n",
        "    one_hot[0, index] = 1\n",
        "    one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "    one_hot = torch.sum(one_hot.cpu() * logits_per_image)\n",
        "    model.zero_grad()\n",
        "    one_hot.backward(retain_graph=True)\n",
        "\n",
        "    image_attn_blocks = list(dict(model.visual.transformer.resblocks.named_children()).values())\n",
        "    num_tokens = image_attn_blocks[0].attn_probs.shape[-1]\n",
        "    R = torch.eye(num_tokens, num_tokens, dtype=image_attn_blocks[0].attn_probs.dtype).to(device)\n",
        "    for blk in image_attn_blocks:\n",
        "        grad = blk.attn_grad\n",
        "        cam = blk.attn_probs\n",
        "        cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "        grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
        "        cam = grad * cam\n",
        "        cam = cam.clamp(min=0).mean(dim=0)\n",
        "        R += torch.matmul(cam, R)\n",
        "    R[0, 0] = 0\n",
        "    image_relevance = R[0, 1:]\n",
        "\n",
        "    # create heatmap from mask on image\n",
        "    def show_cam_on_image(img, mask):\n",
        "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "        heatmap = np.float32(heatmap) / 255\n",
        "        cam = heatmap + np.float32(img)\n",
        "        cam = cam / np.max(cam)\n",
        "        return cam\n",
        "\n",
        "    image_relevance = image_relevance.reshape(1, 1, 7, 7)\n",
        "    image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
        "    image_relevance = image_relevance.reshape(224, 224).cpu().data.numpy()\n",
        "    image_relevance = (image_relevance - image_relevance.min()) / (image_relevance.max() - image_relevance.min())\n",
        "    image = image[0].permute(1, 2, 0).data.cpu().numpy()\n",
        "    image = (image - image.min()) / (image.max() - image.min())\n",
        "    vis = show_cam_on_image(image, image_relevance)\n",
        "    vis = np.uint8(255 * vis)\n",
        "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    plt.imshow(vis)\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-22 12:11:39.443 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run c:\\Users\\born1\\Desktop\\Github\\CLIP-ViL GRADCAM\\CLIP-ViL-GradCAM\\CLIP_virtual_env\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "\n",
        "from torchray.attribution.grad_cam import grad_cam\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "st.sidebar.header('Options')\n",
        "alpha = st.sidebar.radio(\"select alpha\", [0.5, 0.7, 0.8], index=1)\n",
        "layer = st.sidebar.selectbox(\"select saliency layer\", ['layer4.2.relu'], index=0)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_rn, preprocess = clip.load(\"RN50\", device=device, jit=False)\n",
        "\n",
        "def interpret_rn(image, text, model, device, index=None):   \n",
        "    image_features = model.encode_image(image)\n",
        "    text_features = model.encode_text(text)\n",
        "    image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
        "    image_features_new = image_features / image_features_norm\n",
        "    text_features_norm = text_features.norm(dim=-1, keepdim=True)\n",
        "    text_features_new = text_features / text_features_norm\n",
        "    logit_scale = model.logit_scale.exp()\n",
        "    logits_per_image = logit_scale * image_features_new @ text_features_new.t()\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().detach().numpy().tolist()\n",
        "    \n",
        "    text_prediction = (text_features_new * image_features_norm)\n",
        "    image_relevance = grad_cam(model.visual, image.type(model.dtype), text_prediction, saliency_layer=layer)\n",
        "        \n",
        "#     image_relevance = grad_cam(model.visual, image.type(model.dtype), image_features, saliency_layer=layer)\n",
        "\n",
        "    # create heatmap from mask on image\n",
        "    def show_cam_on_image(img, mask):\n",
        "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "        heatmap = np.float32(heatmap) / 255\n",
        "        cam = heatmap + np.float32(img)\n",
        "        cam = cam / np.max(cam)\n",
        "        return cam\n",
        "\n",
        "    image_relevance = image_relevance.reshape(1, 1, 7, 7)\n",
        "    image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
        "    image_relevance = image_relevance.reshape(224, 224).cpu().data.numpy()\n",
        "    image_relevance = (image_relevance - image_relevance.min()) / (image_relevance.max() - image_relevance.min())\n",
        "    image = image[0].permute(1, 2, 0).data.cpu().numpy()\n",
        "    image = (image - image.min()) / (image.max() - image.min())\n",
        "    vis = show_cam_on_image(image, image_relevance)\n",
        "    vis = np.uint8(255 * vis)\n",
        "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    plt.imshow(vis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Fjtms-ps2Anp",
        "outputId": "5ddd9d18-4863-4d55-dd7d-1ca22cfefc5a"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6O0lYX7N2Anq"
      },
      "outputs": [],
      "source": [
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "SlhG0ekE2Anr",
        "outputId": "4733285c-f50b-4c6b-f6b1-2bbf51f8536c"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "too many dimensions 'str'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[38], line 96\u001b[0m\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Danger classification using CLIP on the current frame\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdanger\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Update danger/safe frame counts based on prediction\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predicted_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdanger\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "Cell \u001b[1;32mIn[38], line 69\u001b[0m, in \u001b[0;36mclassify_frame\u001b[1;34m(frame, text_prompts)\u001b[0m\n\u001b[0;32m     66\u001b[0m text \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(text_indices)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Assuming text prompts are already pre-processed with integer indices\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_prompts\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Forward pass through CLIP model (assuming it takes both image and text)\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
            "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
          ]
        }
      ],
      "source": [
        "img_id = 'COCO_val2014_000000393267'\n",
        "MSCOCO_IMG_ROOT = \"/rscratch/data/coco_2014/images\"\n",
        "\n",
        "# COCO_val2014_000000393267 What color is the woman's shirt on the left? {'black': 1, 'blonde': 0.3}\n",
        "import os\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "ori_preprocess = Compose([\n",
        "        Resize((224), interpolation=Image.BICUBIC),\n",
        "    CenterCrop(size=(224, 224)),\n",
        "        ToTensor()])\n",
        "# img_path = os.path.join(MSCOCO_IMG_ROOT, \"val2014\", img_id + \".jpg\")\n",
        "# img_path = 'id.png'\n",
        "# image = ori_preprocess(Image.open(img_path))\n",
        "# print(preprocess)\n",
        "\n",
        "from matplotlib import rc\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------\n",
        "import cv2\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "# Load CLIP model and processor\n",
        "model_name = \"openai/clip-vit-base-patch32\"  # Replace with your chosen CLIP model\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "model = CLIPModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Function for danger classification in a frame\n",
        "def classify_frame(frame, text_prompts):\n",
        "  \"\"\"\n",
        "  Classifies a video frame based on provided text prompts using CLIP.\n",
        "\n",
        "  Args:\n",
        "      frame: The video frame as a NumPy array.\n",
        "      text_prompts: List of text prompts representing danger and safety categories.\n",
        "\n",
        "  Returns:\n",
        "      A string indicating the predicted class (\"danger\" or \"safe\").\n",
        "  \"\"\"\n",
        "  \n",
        "  # Pre-process the frame (resize, normalize) as required by your CLIP model\n",
        "  # ... (refer to your CLIP model's documentation for pre-processing steps)\n",
        "  frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # Resize the frame (check the model's documentation for required dimensions)\n",
        "  if model_name == \"openai/clip-vit-base-patch32\":\n",
        "      # Assuming CLIP-Vit-Base-Patch32 expects 224x224\n",
        "      frame = cv2.resize(frame, (224, 224))\n",
        "  else:\n",
        "      # Modify based on your CLIP model's requirements\n",
        "      raise ValueError(f\"Unsupported CLIP model: {model_name}\")\n",
        "\n",
        "  # Normalize pixel values (assuming model expects values between 0 and 1)\n",
        "  frame = frame.astype(np.float32) / 255.0\n",
        "\n",
        "  # Convert frame to a tensor and transpose dimensions (B, G, R) -> (C, H, W)\n",
        "  frame_tensor = torch.from_numpy(frame).permute(2, 0, 1).unsqueeze(0)\n",
        "\n",
        "  \n",
        "  # Convert frame to a tensor and move it to device\n",
        "  #frame_tensor = ...  # Conversion based on your pre-processing steps (e.g., to torch.tensor)\n",
        "  frame_tensor = frame_tensor.to(device)\n",
        "\n",
        "  vocabulary={\"danger\":1,\"safe\":2}\n",
        "\n",
        "  # Convert text prompts to integer indices (assuming a vocabulary exists)\n",
        "  text_indices = [vocabulary[prompt] for prompt in text_prompts]\n",
        "  text = torch.tensor(text_indices).to(device)\n",
        "\n",
        "  # Assuming text prompts are already pre-processed with integer indices\n",
        "  text = torch.tensor(text_prompts).to(device)\n",
        "\n",
        "  # Forward pass through CLIP model (assuming it takes both image and text)\n",
        "  with torch.no_grad():\n",
        "      logits = model(frame_tensor, text)\n",
        "\n",
        "  # Get the class with the highest score (assuming higher logits indicate danger)\n",
        "  predicted_class = torch.argmax(logits).item()\n",
        "  predicted_class_text = text_prompts[predicted_class]\n",
        "\n",
        "  return predicted_class_text\n",
        "\n",
        "# Video processing loop\n",
        "video_path = \"dangerous accident Very lucky Survival cctv camera.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX  # Font for danger label\n",
        "\n",
        "danger_frame_count = 0\n",
        "safe_frame_count = 0\n",
        "\n",
        "while True:\n",
        "  ret, frame = cap.read()\n",
        "  if not ret:\n",
        "    break\n",
        "\n",
        "  # Danger classification using CLIP on the current frame\n",
        "  predicted_class = classify_frame(frame, [\"danger\", \"safe\"])\n",
        "\n",
        "  # Update danger/safe frame counts based on prediction\n",
        "  if predicted_class == \"danger\":\n",
        "      danger_frame_count += 1\n",
        "      # Add danger label to the frame\n",
        "      cv2.putText(frame, \"Danger!\", (10, 30), font, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "  else:\n",
        "      safe_frame_count += 1\n",
        "\n",
        "  # Display the frame with potential danger label\n",
        "  cv2.imshow(\"Video\", frame)\n",
        "\n",
        "  # Exit on 'q' key press\n",
        "  if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "      break\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# font = {\n",
        "#     'size': 32,\n",
        "# }\n",
        "# import matplotlib\n",
        "# matplotlib.rcParams['mathtext.fontset'] = 'custom'\n",
        "# matplotlib.rcParams['mathtext.rm'] = 'Bitstream Vera Sans'\n",
        "# matplotlib.rcParams['mathtext.it'] = 'Bitstream Vera Sans:italic'\n",
        "# matplotlib.rcParams['mathtext.bf'] = 'Bitstream Vera Sans:bold'\n",
        "# # matplotlib.rcParams['mathtext.size'] = 16\n",
        "\n",
        "# # {'cursive', 'fantasy', 'monospace', 'sans', 'sans serif', 'sans-serif', 'serif'}\n",
        "# plt.figure(figsize=(16, 16))\n",
        "# plt.tight_layout()\n",
        "# plt.subplot(131)\n",
        "# plt.imshow(image.permute(1, 2, 0))\n",
        "# plt.axis('off')\n",
        "# plt.title(\"(a) Original\", **font, y=-0.15)\n",
        "\n",
        "# # plt.savefig('/rscratch/sheng.s/clip_boi/clip_vqa_starting/visual/sample_1_ori.pdf', bbox_inches='tight')\n",
        "# # plt.show()\n",
        "# texts = [\"face\"]\n",
        "\n",
        "\n",
        "# image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "# text = clip.tokenize(texts).to(device)\n",
        "# print(color.BOLD + color.PURPLE + color.UNDERLINE + 'text: ' + texts[0] + color.END)\n",
        "# plt.subplot(132)\n",
        "# plt.axis('off')\n",
        "# plt.title(\"(b) ViT-B/32\", **font,y=-0.15)\n",
        "# interpret_vit(model=model, image=image, text=text, device=device, index=0)\n",
        "# plt.subplot(133)\n",
        "# plt.axis('off')\n",
        "# plt.title(\"(c) RN50\", **font,y=-0.15)\n",
        "# interpret_rn(model=model_rn, image=image, text=text, device=device, index=0)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "text = clip.tokenize(texts).to(device)\n",
        "plt.subplot(133)\n",
        "print(color.BOLD + color.PURPLE + color.UNDERLINE + 'text: ' + texts[0] + color.END)\n",
        "interpret_rn(model=model_rn, image=image, text=text, device=device, index=0)\n",
        "plt.axis('off')\n",
        "plt.title(\"(c) RN50\", **font,y=-0.15)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('sample_all.pdf', bbox_inches='tight')\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "images = []\n",
        "from glob import glob\n",
        "\n",
        "image_files = glob('./bottle/*.jpg')\n",
        "\n",
        "# Load and store images in a list\n",
        "images = [preprocess(Image.open(file)).unsqueeze(0).to(device) for file in image_files]\n",
        "images = np.stack(images)\n",
        "images = torch.Tensor(images).squeeze(1)\n",
        "texts= [\"bottle\", \"mom\", \"lion\"]\n",
        "text = clip.tokenize(texts).to(device)\n",
        "logits_per_image, logits_per_text = model(images, text)\n",
        "probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[9.80874002e-01, 1.74526330e-02, 1.67341577e-03],\n",
              "       [9.95607436e-01, 3.55899474e-03, 8.33532307e-04],\n",
              "       [9.92620230e-01, 6.58456888e-03, 7.95182423e-04],\n",
              "       [9.92818892e-01, 5.86325396e-03, 1.31783565e-03],\n",
              "       [9.93724763e-01, 4.45985748e-03, 1.81542360e-03],\n",
              "       [9.91071224e-01, 6.50541391e-03, 2.42342823e-03],\n",
              "       [9.91384864e-01, 5.98575547e-03, 2.62938719e-03],\n",
              "       [9.93631899e-01, 5.23412926e-03, 1.13408861e-03],\n",
              "       [9.76863801e-01, 2.09404062e-02, 2.19580322e-03],\n",
              "       [9.89538431e-01, 8.15730635e-03, 2.30429950e-03],\n",
              "       [9.91728187e-01, 7.03415601e-03, 1.23772700e-03],\n",
              "       [9.90314782e-01, 8.53700563e-03, 1.14823144e-03],\n",
              "       [9.93409157e-01, 5.49768517e-03, 1.09319668e-03],\n",
              "       [9.91212189e-01, 6.31303200e-03, 2.47475691e-03],\n",
              "       [9.95156586e-01, 3.37180356e-03, 1.47161086e-03],\n",
              "       [9.93618011e-01, 4.93897311e-03, 1.44309725e-03],\n",
              "       [9.95293438e-01, 3.74574005e-03, 9.60841309e-04],\n",
              "       [9.94298518e-01, 4.70601907e-03, 9.95490584e-04],\n",
              "       [9.95471001e-01, 3.49248922e-03, 1.03656272e-03],\n",
              "       [9.95038450e-01, 4.16872883e-03, 7.92899635e-04],\n",
              "       [9.95687187e-01, 3.21765197e-03, 1.09508296e-03],\n",
              "       [9.94956315e-01, 3.79403331e-03, 1.24964165e-03],\n",
              "       [9.90373492e-01, 6.12037862e-03, 3.50617431e-03],\n",
              "       [9.95053113e-01, 3.30153573e-03, 1.64536666e-03],\n",
              "       [9.94410098e-01, 2.86809611e-03, 2.72177462e-03],\n",
              "       [9.96098042e-01, 1.95141195e-03, 1.95043720e-03],\n",
              "       [9.94765878e-01, 3.68605740e-03, 1.54805591e-03],\n",
              "       [9.93598700e-01, 4.95427474e-03, 1.44702161e-03],\n",
              "       [9.94657576e-01, 4.48827399e-03, 8.54042359e-04],\n",
              "       [9.88574147e-01, 9.31785256e-03, 2.10801582e-03],\n",
              "       [9.91768479e-01, 6.08444726e-03, 2.14698398e-03],\n",
              "       [9.90143538e-01, 5.84043143e-03, 4.01601894e-03],\n",
              "       [9.83229697e-01, 7.79135711e-03, 8.97895638e-03],\n",
              "       [9.89961028e-01, 5.93226170e-03, 4.10672883e-03],\n",
              "       [9.95217204e-01, 3.43864039e-03, 1.34422246e-03],\n",
              "       [9.94272232e-01, 2.68662092e-03, 3.04120639e-03],\n",
              "       [9.94598150e-01, 1.71630760e-03, 3.68543598e-03],\n",
              "       [9.95178103e-01, 1.20095955e-03, 3.62100219e-03],\n",
              "       [9.90583181e-01, 2.81944056e-03, 6.59741648e-03],\n",
              "       [9.89173770e-01, 1.53984979e-03, 9.28628724e-03],\n",
              "       [9.93068814e-01, 1.39291573e-03, 5.53835649e-03],\n",
              "       [9.93589163e-01, 1.50198909e-03, 4.90882387e-03],\n",
              "       [9.96528327e-01, 1.46533747e-03, 2.00638874e-03],\n",
              "       [9.96452332e-01, 1.39868283e-03, 2.14898027e-03],\n",
              "       [9.96662796e-01, 1.37791981e-03, 1.95914367e-03],\n",
              "       [9.91267383e-01, 7.37895351e-03, 1.35374255e-03],\n",
              "       [9.97446299e-01, 1.12557597e-03, 1.42811972e-03],\n",
              "       [9.96693373e-01, 1.35150319e-03, 1.95508637e-03],\n",
              "       [9.97680902e-01, 1.25371630e-03, 1.06542860e-03],\n",
              "       [9.97787714e-01, 1.44330261e-03, 7.68999686e-04],\n",
              "       [9.97274816e-01, 1.89403770e-03, 8.31170473e-04],\n",
              "       [9.96901274e-01, 2.33668229e-03, 7.62129494e-04],\n",
              "       [9.94216144e-01, 3.91800050e-03, 1.86580664e-03],\n",
              "       [9.97404873e-01, 1.67047011e-03, 9.24666820e-04],\n",
              "       [9.96046007e-01, 3.33190663e-03, 6.22122141e-04],\n",
              "       [9.95233834e-01, 4.03268682e-03, 7.33449298e-04],\n",
              "       [9.91778016e-01, 7.09746126e-03, 1.12454186e-03],\n",
              "       [9.97135162e-01, 2.20985082e-03, 6.54917327e-04],\n",
              "       [9.92238104e-01, 6.15505595e-03, 1.60687696e-03],\n",
              "       [9.95176911e-01, 4.07604594e-03, 7.47009995e-04],\n",
              "       [9.97680008e-01, 1.92147342e-03, 3.98507924e-04],\n",
              "       [9.92901623e-01, 5.91944531e-03, 1.17888441e-03],\n",
              "       [9.92036283e-01, 7.05765886e-03, 9.06079134e-04],\n",
              "       [9.95971859e-01, 3.48865939e-03, 5.39521337e-04],\n",
              "       [9.97225404e-01, 2.40507605e-03, 3.69578920e-04],\n",
              "       [9.97114182e-01, 2.31614639e-03, 5.69663360e-04],\n",
              "       [9.97212231e-01, 1.95737672e-03, 8.30338744e-04],\n",
              "       [9.77736175e-01, 1.99072566e-02, 2.35646754e-03],\n",
              "       [9.95084107e-01, 4.52684844e-03, 3.88915127e-04],\n",
              "       [9.96592820e-01, 3.15249129e-03, 2.54659768e-04],\n",
              "       [9.97105658e-01, 2.46043969e-03, 4.33879468e-04],\n",
              "       [9.95439708e-01, 3.42236226e-03, 1.13798457e-03],\n",
              "       [9.95932281e-01, 3.25193163e-03, 8.15796782e-04],\n",
              "       [9.93090749e-01, 6.33645570e-03, 5.72754361e-04],\n",
              "       [9.86405909e-01, 1.24198608e-02, 1.17418705e-03],\n",
              "       [9.88636792e-01, 1.00905253e-02, 1.27274939e-03],\n",
              "       [9.90817547e-01, 6.60761725e-03, 2.57488783e-03],\n",
              "       [9.96585011e-01, 2.78857630e-03, 6.26469147e-04],\n",
              "       [9.93906438e-01, 5.22035919e-03, 8.73162004e-04],\n",
              "       [9.96428430e-01, 2.92008859e-03, 6.51479058e-04],\n",
              "       [9.93680000e-01, 5.09945489e-03, 1.22055179e-03],\n",
              "       [9.92650568e-01, 5.63407876e-03, 1.71531714e-03],\n",
              "       [9.88927007e-01, 8.58735014e-03, 2.48569855e-03],\n",
              "       [9.92447257e-01, 5.55839716e-03, 1.99431530e-03],\n",
              "       [9.91193831e-01, 6.44313823e-03, 2.36311159e-03],\n",
              "       [9.93491113e-01, 4.17165691e-03, 2.33712117e-03],\n",
              "       [9.94924545e-01, 4.05477639e-03, 1.02071057e-03],\n",
              "       [9.96533513e-01, 2.48019653e-03, 9.86298081e-04],\n",
              "       [9.96743381e-01, 2.27922481e-03, 9.77479853e-04],\n",
              "       [9.94789839e-01, 4.48650820e-03, 7.23571400e-04],\n",
              "       [9.94286478e-01, 4.56311507e-03, 1.15047523e-03],\n",
              "       [9.94182527e-01, 5.27497521e-03, 5.42494410e-04],\n",
              "       [9.89079654e-01, 1.00737140e-02, 8.46732990e-04],\n",
              "       [9.94991362e-01, 4.11031255e-03, 8.98423663e-04],\n",
              "       [9.97078061e-01, 2.40546395e-03, 5.16439555e-04],\n",
              "       [9.93193567e-01, 6.22337777e-03, 5.83167421e-04],\n",
              "       [9.95310128e-01, 3.97737464e-03, 7.12501758e-04],\n",
              "       [9.95577574e-01, 3.75661976e-03, 6.65864674e-04],\n",
              "       [9.90945101e-01, 6.74987119e-03, 2.30505457e-03],\n",
              "       [9.93824601e-01, 4.72546555e-03, 1.44994794e-03],\n",
              "       [9.93585646e-01, 5.10051660e-03, 1.31394970e-03],\n",
              "       [9.94768858e-01, 3.44358757e-03, 1.78751175e-03],\n",
              "       [9.95569646e-01, 1.60828861e-03, 2.82212836e-03],\n",
              "       [9.85225141e-01, 1.37726758e-02, 1.00222672e-03],\n",
              "       [9.95556533e-01, 2.62645259e-03, 1.81698089e-03],\n",
              "       [9.96133447e-01, 2.46647559e-03, 1.40004465e-03],\n",
              "       [9.94547367e-01, 4.27286699e-03, 1.17973762e-03],\n",
              "       [9.95814502e-01, 2.25894060e-03, 1.92652561e-03],\n",
              "       [9.96417046e-01, 2.00938154e-03, 1.57348288e-03],\n",
              "       [9.88514364e-01, 8.58319458e-03, 2.90239323e-03],\n",
              "       [9.92002964e-01, 5.01916371e-03, 2.97789392e-03],\n",
              "       [9.93758082e-01, 3.39872018e-03, 2.84316763e-03],\n",
              "       [9.94791031e-01, 2.61783367e-03, 2.59113777e-03],\n",
              "       [9.91904438e-01, 3.34600313e-03, 4.74963756e-03],\n",
              "       [9.88277972e-01, 1.03150820e-02, 1.40690850e-03],\n",
              "       [9.91567552e-01, 4.63562226e-03, 3.79685126e-03],\n",
              "       [9.94596601e-01, 3.05027468e-03, 2.35301792e-03],\n",
              "       [9.94430602e-01, 3.17185954e-03, 2.39748973e-03],\n",
              "       [9.92762625e-01, 4.07646177e-03, 3.16092139e-03],\n",
              "       [9.92093742e-01, 4.48652636e-03, 3.41964629e-03],\n",
              "       [9.94611502e-01, 3.31810908e-03, 2.07042438e-03],\n",
              "       [9.95318055e-01, 3.29472031e-03, 1.38728181e-03],\n",
              "       [9.95144784e-01, 3.33622075e-03, 1.51895860e-03],\n",
              "       [9.95760381e-01, 2.67052837e-03, 1.56909938e-03],\n",
              "       [9.95584667e-01, 3.10784788e-03, 1.30756409e-03],\n",
              "       [9.85252142e-01, 1.40528753e-02, 6.94942020e-04],\n",
              "       [9.93499875e-01, 4.54579387e-03, 1.95439812e-03],\n",
              "       [9.96410549e-01, 1.58889568e-03, 2.00050045e-03],\n",
              "       [9.96954560e-01, 1.89795648e-03, 1.14746613e-03],\n",
              "       [9.95943129e-01, 2.12069345e-03, 1.93613174e-03],\n",
              "       [9.94503260e-01, 2.63344008e-03, 2.86330353e-03],\n",
              "       [9.95148718e-01, 3.67929135e-03, 1.17190136e-03],\n",
              "       [9.96416926e-01, 2.50812434e-03, 1.07500283e-03],\n",
              "       [9.93369281e-01, 5.75135741e-03, 8.79328407e-04],\n",
              "       [9.95941699e-01, 3.34628811e-03, 7.11963221e-04],\n",
              "       [9.94374037e-01, 4.72562574e-03, 9.00345971e-04],\n",
              "       [9.87214327e-01, 1.19378110e-02, 8.47767806e-04],\n",
              "       [9.95250940e-01, 3.94696370e-03, 8.01982824e-04],\n",
              "       [9.95814621e-01, 3.56576289e-03, 6.19506696e-04],\n",
              "       [9.96947229e-01, 2.54523661e-03, 5.07628778e-04],\n",
              "       [9.98756886e-01, 1.03366782e-03, 2.09458725e-04],\n",
              "       [9.97214258e-01, 2.30484991e-03, 4.81035531e-04],\n",
              "       [9.97715116e-01, 1.90818089e-03, 3.76617740e-04],\n",
              "       [9.97923851e-01, 1.80872111e-03, 2.67433410e-04],\n",
              "       [9.97373939e-01, 2.14213273e-03, 4.83882643e-04],\n",
              "       [9.96223092e-01, 2.66252086e-03, 1.11434131e-03],\n",
              "       [9.97291148e-01, 2.06839782e-03, 6.40416460e-04],\n",
              "       [9.86015201e-01, 1.31945545e-02, 7.90200080e-04],\n",
              "       [9.95264888e-01, 4.15081624e-03, 5.84365742e-04],\n",
              "       [9.96205807e-01, 3.42477439e-03, 3.69277142e-04],\n",
              "       [9.94176030e-01, 5.24087297e-03, 5.83097746e-04],\n",
              "       [9.95480418e-01, 3.91583610e-03, 6.03684864e-04],\n",
              "       [9.93434191e-01, 5.51674189e-03, 1.04903989e-03],\n",
              "       [9.93154287e-01, 6.26975158e-03, 5.75991173e-04],\n",
              "       [9.92174923e-01, 7.30849570e-03, 5.16491244e-04],\n",
              "       [9.91528749e-01, 7.75641995e-03, 7.14853057e-04],\n",
              "       [9.95081663e-01, 3.57896090e-03, 1.33933581e-03],\n",
              "       [9.96581852e-01, 2.75603752e-03, 6.62136939e-04],\n",
              "       [9.91234660e-01, 7.79831829e-03, 9.66970110e-04],\n",
              "       [9.94735420e-01, 4.23757918e-03, 1.02687220e-03],\n",
              "       [9.94906068e-01, 3.89380101e-03, 1.20017771e-03],\n",
              "       [9.91322279e-01, 6.76316395e-03, 1.91451993e-03],\n",
              "       [9.86103535e-01, 1.11408923e-02, 2.75558210e-03],\n",
              "       [9.87939358e-01, 8.61366466e-03, 3.44695966e-03],\n",
              "       [9.89922464e-01, 7.76124373e-03, 2.31622532e-03],\n",
              "       [9.93884385e-01, 4.59549250e-03, 1.52010785e-03],\n",
              "       [9.94965613e-01, 4.14641947e-03, 8.87896400e-04],\n",
              "       [9.94548500e-01, 4.31435928e-03, 1.13710237e-03],\n",
              "       [9.93571401e-01, 5.63141610e-03, 7.97266257e-04],\n",
              "       [9.93198931e-01, 5.38042048e-03, 1.42062886e-03],\n",
              "       [9.91908133e-01, 6.96368702e-03, 1.12812268e-03],\n",
              "       [9.92016792e-01, 7.09796557e-03, 8.85325717e-04],\n",
              "       [9.92587984e-01, 6.55538356e-03, 8.56713101e-04],\n",
              "       [9.89730358e-01, 9.41367075e-03, 8.55950988e-04],\n",
              "       [9.92174029e-01, 7.07649998e-03, 7.49516999e-04],\n",
              "       [9.94658768e-01, 4.75013396e-03, 5.91131859e-04],\n",
              "       [9.92521048e-01, 6.93877274e-03, 5.40244509e-04],\n",
              "       [9.87068415e-01, 1.25699779e-02, 3.61546612e-04],\n",
              "       [9.85923052e-01, 1.33326566e-02, 7.44360266e-04],\n",
              "       [9.91992474e-01, 7.25352345e-03, 7.54036708e-04]], dtype=float32)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "gradcam_clip.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
